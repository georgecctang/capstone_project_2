{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Dates\n",
    "\n",
    "Num Training Weeks = 4  \n",
    "Num Test/Val Days = 15\n",
    "\n",
    "- Train: 5/30/2017 (Tues), 6/6/2017,6/13/2017,6/20/2017  \n",
    "- Val: 7/11/2017 (Tues) - 7/25/2017 (Wed)  \n",
    "- Test: 8/1/2017 (Tues)  - 8/15/2017 (Wed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sales data from 2017-01-01 and beyond\n",
    "\n",
    "df_2017 = pd.read_csv(\n",
    "    '../input/train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 101688780))\n",
    "\n",
    "\n",
    "# import items and stores data\n",
    "items = pd.read_csv(\"../input/items.csv\").set_index(\"item_nbr\")\n",
    "stores = pd.read_csv(\"../input/stores.csv\").set_index(\"store_nbr\")\n",
    "\n",
    "# Create promotion dataset\n",
    "promo_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017.columns = promo_2017.columns.get_level_values(1)\n",
    "\n",
    "# Transform sales training data\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "df_2017_index = df_2017.index\n",
    "\n",
    "# transform items dataset\n",
    "items['class'] = items['class'].astype('category')\n",
    "items = pd.get_dummies(items)\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "# transform \n",
    "stores['cluster'] = stores.cluster.astype('category')\n",
    "stores = pd.get_dummies(stores)\n",
    "stores = stores.reindex(df_2017.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(df, promo_df, t2017, is_train=True, name_prefix=None):\n",
    "    X = {\n",
    "        \n",
    "        # Number of promotion in the last x days \n",
    "        \"promo_7_2017\": get_timespan(promo_df, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_df, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_30_2017\": get_timespan(promo_df, t2017, 30, 30).sum(axis=1).values,\n",
    "        \n",
    "        # Number of promotion in the next x days of reference date\n",
    "        \"promo_3_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 3).sum(axis=1).values,\n",
    "        \"promo_7_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14).sum(axis=1).values,\n",
    "    }\n",
    "    \n",
    "# Removed due to the presence of nan values \n",
    "#     for i in [3, 7, 14, 30]:\n",
    "#         tmp1 = get_timespan(df, t2017, i, i)\n",
    "#         tmp2 = (get_timespan(promo_df, t2017, i, i) > 0) * 1\n",
    "\n",
    "#         X['has_promo_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n",
    "#         X['no_promo_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).values\n",
    "    \n",
    "    for i in [3, 7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # mean daily difference in sales in the last x days\n",
    "        X[f'diff_{i}_mean'] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        # mean sales in the last x days\n",
    "        X[f'mean_{i}'] = tmp.mean(axis=1).values\n",
    "        # median sales in the last x days\n",
    "        X[f'median_{i}'] = tmp.median(axis=1).values\n",
    "        # min sales in the last x days\n",
    "        X[f'min_{i}'] = tmp.min(axis=1).values\n",
    "        # max sales in the last x days\n",
    "        X[f'max_{i}'] = tmp.max(axis=1).values\n",
    "        # std dev sales in the last x days\n",
    "        X[f'std_{i}'] = tmp.std(axis=1).values\n",
    "\n",
    "    for i in [7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # number of days with sales in the last x days\n",
    "        X[f'has_sales_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day with sales in the last x days\n",
    "        X[f'last_has_sales_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first of days with sales in the last x days\n",
    "        X[f'first_has_sales_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "        tmp = get_timespan(promo_df, t2017, i, i)\n",
    "        # number of days with promotions in the last x days\n",
    "        X[f'has_promo_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day has promotion in the last x days\n",
    "        X[f'last_has_promo_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first day has promotion in the last x days\n",
    "        X[f'first_has_promo_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "    tmp = get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14)\n",
    "    # last day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    # Count backwards: if last day is 8/15, then value = 1\n",
    "    # if last day is 8/2, then value = 14\n",
    "    X['last_has_promo_day_in_after_14_days'] = 14 - ((tmp > 0) * np.arange(14)).max(axis=1).values\n",
    "    # first day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    X['first_has_promo_day_in_after_14_days'] = ((tmp > 0) * np.arange(14, 0, -1)).max(axis=1).values\n",
    "\n",
    "    # sale on day x days from reference date \n",
    "    for i in range(1, 15):\n",
    "        X[f'day_{i}_2017'] = get_timespan(df, t2017, i, 1).values.ravel()\n",
    "    \n",
    "    # average sales on day of the week for the last 4 or 20 weeks\n",
    "    for i in range(7):\n",
    "        X[f'mean_4_dow{i}_2017'] = get_timespan(df, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X[f'mean_20_dow{i}_2017'] = get_timespan(df, t2017, 140-i, 20, freq='7D').mean(axis=1).values        \n",
    "    \n",
    "    # promotion status of each day 14 days before and 14 days after the reference date\n",
    "    \n",
    "    for i in range(-14, 15):\n",
    "        X[f'promo_{i}'] = promo_df[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    if name_prefix is not None:\n",
    "        X.columns = [f'{name_prefix}_{c}' for c in X.columns]\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(t2017, periods=15)].values\n",
    "        return X, y\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create training data\n",
    "\n",
    "t2017 = date(2017, 5, 30)\n",
    "num_days = 4\n",
    "X_l, y_l = [], []\n",
    "for i in tqdm(range(num_days)):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(df_2017, promo_2017, t2017 + delta)\n",
    "\n",
    "    X_tmp = pd.concat([X_tmp, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "\n",
    "del X_l, y_l\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data\n",
    "\n",
    "X_val, y_val = prepare_dataset(df_2017, promo_2017, date(2017, 7, 11))\n",
    "\n",
    "X_val = pd.concat([X_val, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_test, y_test = prepare_dataset(df_2017, promo_2017, date(2017, 8, 1))\n",
    "\n",
    "X_test = pd.concat([X_test, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670060, 538) (670060, 15)\n",
      "(167515, 538) (167515, 15)\n",
      "(167515, 538) (167515, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_2017, promo_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_test[:] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "X_val = X_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256 x 128 x 64 x 32 x 16 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5186 - mse: 0.4869 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3538 - mse: 0.3337 - val_loss: 0.3442 - val_mse: 0.3442\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3383 - mse: 0.3195 - val_loss: 0.3352 - val_mse: 0.3352\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3302 - mse: 0.3121 - val_loss: 0.3259 - val_mse: 0.3259\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3245 - mse: 0.3069 - val_loss: 0.3236 - val_mse: 0.3236\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3207 - mse: 0.3034 - val_loss: 0.3212 - val_mse: 0.3212\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3174 - mse: 0.3003 - val_loss: 0.3206 - val_mse: 0.3206\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3148 - mse: 0.2979 - val_loss: 0.3194 - val_mse: 0.3194\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3121 - mse: 0.2954 - val_loss: 0.3180 - val_mse: 0.3180\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3097 - mse: 0.2932 - val_loss: 0.3181 - val_mse: 0.3181\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3079 - mse: 0.2915 - val_loss: 0.3210 - val_mse: 0.3210\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3061 - mse: 0.2899 - val_loss: 0.3186 - val_mse: 0.3186\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3039 - mse: 0.2878 - val_loss: 0.3222 - val_mse: 0.3222\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3023 - mse: 0.2863 - val_loss: 0.3230 - val_mse: 0.3230\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3005 - mse: 0.2847 - val_loss: 0.3230 - val_mse: 0.3230\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.2994 - mse: 0.2837 - val_loss: 0.3224 - val_mse: 0.3224\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/2000\n",
      " - 8s - loss: 0.2882 - mse: 0.2734 - val_loss: 0.3203 - val_mse: 0.3203\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.2843 - mse: 0.2698 - val_loss: 0.3208 - val_mse: 0.3208\n",
      "Epoch 19/2000\n",
      " - 9s - loss: 0.2828 - mse: 0.2684 - val_loss: 0.3219 - val_mse: 0.3219\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5407 - mse: 0.5092 - val_loss: 0.4184 - val_mse: 0.4184\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3611 - mse: 0.3420 - val_loss: 0.3507 - val_mse: 0.3507\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3424 - mse: 0.3250 - val_loss: 0.3326 - val_mse: 0.3326\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3333 - mse: 0.3166 - val_loss: 0.3270 - val_mse: 0.3270\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3264 - mse: 0.3103 - val_loss: 0.3264 - val_mse: 0.3264\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3219 - mse: 0.3062 - val_loss: 0.3222 - val_mse: 0.3222\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3182 - mse: 0.3028 - val_loss: 0.3222 - val_mse: 0.3222\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3152 - mse: 0.3000 - val_loss: 0.3206 - val_mse: 0.3206\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3126 - mse: 0.2975 - val_loss: 0.3204 - val_mse: 0.3204\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3100 - mse: 0.2951 - val_loss: 0.3208 - val_mse: 0.3208\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3084 - mse: 0.2936 - val_loss: 0.3204 - val_mse: 0.3204\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3061 - mse: 0.2915 - val_loss: 0.3229 - val_mse: 0.3229\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3044 - mse: 0.2899 - val_loss: 0.3226 - val_mse: 0.3226\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3022 - mse: 0.2878 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3010 - mse: 0.2867 - val_loss: 0.3252 - val_mse: 0.3252\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.2991 - mse: 0.2848 - val_loss: 0.3255 - val_mse: 0.3255\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.2881 - mse: 0.2747 - val_loss: 0.3225 - val_mse: 0.3225\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.2845 - mse: 0.2713 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 19/2000\n",
      " - 9s - loss: 0.2829 - mse: 0.2699 - val_loss: 0.3243 - val_mse: 0.3243\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5757 - mse: 0.5422 - val_loss: 0.4008 - val_mse: 0.4008\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3820 - mse: 0.3609 - val_loss: 0.3648 - val_mse: 0.3648\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3672 - mse: 0.3471 - val_loss: 0.3535 - val_mse: 0.3535\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3590 - mse: 0.3394 - val_loss: 0.3465 - val_mse: 0.3465\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3532 - mse: 0.3339 - val_loss: 0.3461 - val_mse: 0.3461\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3484 - mse: 0.3295 - val_loss: 0.3452 - val_mse: 0.3452\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3447 - mse: 0.3260 - val_loss: 0.3497 - val_mse: 0.3497\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3417 - mse: 0.3232 - val_loss: 0.3462 - val_mse: 0.3462\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3389 - mse: 0.3206 - val_loss: 0.3481 - val_mse: 0.3481\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3369 - mse: 0.3187 - val_loss: 0.3472 - val_mse: 0.3472\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3344 - mse: 0.3164 - val_loss: 0.3486 - val_mse: 0.3486\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3324 - mse: 0.3146 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3300 - mse: 0.3123 - val_loss: 0.3529 - val_mse: 0.3529\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3201 - mse: 0.3032 - val_loss: 0.3478 - val_mse: 0.3478\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3169 - mse: 0.3002 - val_loss: 0.3488 - val_mse: 0.3488\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3156 - mse: 0.2990 - val_loss: 0.3499 - val_mse: 0.3499\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.4817 - mse: 0.4546 - val_loss: 0.4182 - val_mse: 0.4182\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3732 - mse: 0.3537 - val_loss: 0.3629 - val_mse: 0.3629\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3587 - mse: 0.3405 - val_loss: 0.3542 - val_mse: 0.3542\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3510 - mse: 0.3332 - val_loss: 0.3468 - val_mse: 0.3468\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3457 - mse: 0.3283 - val_loss: 0.3427 - val_mse: 0.3427\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3413 - mse: 0.3242 - val_loss: 0.3424 - val_mse: 0.3424\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3380 - mse: 0.3211 - val_loss: 0.3432 - val_mse: 0.3432\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3344 - mse: 0.3178 - val_loss: 0.3457 - val_mse: 0.3457\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3316 - mse: 0.3152 - val_loss: 0.3447 - val_mse: 0.3447\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3293 - mse: 0.3131 - val_loss: 0.3453 - val_mse: 0.3453\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3267 - mse: 0.3107 - val_loss: 0.3447 - val_mse: 0.3447\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3243 - mse: 0.3084 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3226 - mse: 0.3068 - val_loss: 0.3487 - val_mse: 0.3487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3111 - mse: 0.2962 - val_loss: 0.3436 - val_mse: 0.3436\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3072 - mse: 0.2926 - val_loss: 0.3448 - val_mse: 0.3448\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3056 - mse: 0.2911 - val_loss: 0.3462 - val_mse: 0.3462\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5724 - mse: 0.5398 - val_loss: 0.4776 - val_mse: 0.4776\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4026 - mse: 0.3808 - val_loss: 0.4024 - val_mse: 0.4024\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3865 - mse: 0.3657 - val_loss: 0.3804 - val_mse: 0.3804\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3774 - mse: 0.3572 - val_loss: 0.3727 - val_mse: 0.3727\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3709 - mse: 0.3512 - val_loss: 0.3670 - val_mse: 0.3670\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3661 - mse: 0.3467 - val_loss: 0.3681 - val_mse: 0.3681\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3617 - mse: 0.3427 - val_loss: 0.3649 - val_mse: 0.3649\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3579 - mse: 0.3391 - val_loss: 0.3652 - val_mse: 0.3652\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3549 - mse: 0.3363 - val_loss: 0.3683 - val_mse: 0.3683\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3518 - mse: 0.3334 - val_loss: 0.3700 - val_mse: 0.3700\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3491 - mse: 0.3309 - val_loss: 0.3712 - val_mse: 0.3712\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3468 - mse: 0.3288 - val_loss: 0.3748 - val_mse: 0.3748\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3444 - mse: 0.3266 - val_loss: 0.3703 - val_mse: 0.3703\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3421 - mse: 0.3244 - val_loss: 0.3731 - val_mse: 0.3731\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3300 - mse: 0.3132 - val_loss: 0.3700 - val_mse: 0.3700\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3259 - mse: 0.3094 - val_loss: 0.3707 - val_mse: 0.3707\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3242 - mse: 0.3078 - val_loss: 0.3722 - val_mse: 0.3722\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5688 - mse: 0.5362 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4110 - mse: 0.3885 - val_loss: 0.4068 - val_mse: 0.4068\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3951 - mse: 0.3738 - val_loss: 0.3856 - val_mse: 0.3856\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3857 - mse: 0.3651 - val_loss: 0.3837 - val_mse: 0.3837\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3788 - mse: 0.3588 - val_loss: 0.3823 - val_mse: 0.3823\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3735 - mse: 0.3539 - val_loss: 0.3777 - val_mse: 0.3777\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3690 - mse: 0.3497 - val_loss: 0.3774 - val_mse: 0.3774\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3650 - mse: 0.3460 - val_loss: 0.3785 - val_mse: 0.3785\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3615 - mse: 0.3428 - val_loss: 0.3817 - val_mse: 0.3817\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3585 - mse: 0.3399 - val_loss: 0.3794 - val_mse: 0.3794\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3557 - mse: 0.3373 - val_loss: 0.3813 - val_mse: 0.3813\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3532 - mse: 0.3350 - val_loss: 0.3825 - val_mse: 0.3825\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3504 - mse: 0.3325 - val_loss: 0.3798 - val_mse: 0.3798\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3483 - mse: 0.3304 - val_loss: 0.3848 - val_mse: 0.3848\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3361 - mse: 0.3192 - val_loss: 0.3801 - val_mse: 0.3801\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3320 - mse: 0.3154 - val_loss: 0.3816 - val_mse: 0.3816\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3303 - mse: 0.3139 - val_loss: 0.3825 - val_mse: 0.3825\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5761 - mse: 0.5422 - val_loss: 0.4554 - val_mse: 0.4554\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4030 - mse: 0.3799 - val_loss: 0.3950 - val_mse: 0.3950\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3867 - mse: 0.3646 - val_loss: 0.3769 - val_mse: 0.3769\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3778 - mse: 0.3564 - val_loss: 0.3744 - val_mse: 0.3744\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3721 - mse: 0.3511 - val_loss: 0.3717 - val_mse: 0.3717\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3675 - mse: 0.3469 - val_loss: 0.3721 - val_mse: 0.3721\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3631 - mse: 0.3428 - val_loss: 0.3716 - val_mse: 0.3716\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3598 - mse: 0.3398 - val_loss: 0.3730 - val_mse: 0.3730\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3567 - mse: 0.3369 - val_loss: 0.3724 - val_mse: 0.3724\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3542 - mse: 0.3346 - val_loss: 0.3746 - val_mse: 0.3746\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3519 - mse: 0.3325 - val_loss: 0.3758 - val_mse: 0.3758\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3495 - mse: 0.3303 - val_loss: 0.3757 - val_mse: 0.3757\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3387 - mse: 0.3203 - val_loss: 0.3740 - val_mse: 0.3740\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3351 - mse: 0.3170 - val_loss: 0.3753 - val_mse: 0.3753\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3336 - mse: 0.3156 - val_loss: 0.3753 - val_mse: 0.3753\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3324 - mse: 0.3145 - val_loss: 0.3759 - val_mse: 0.3759\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3317 - mse: 0.3139 - val_loss: 0.3765 - val_mse: 0.3765\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5530 - mse: 0.5206 - val_loss: 0.6000 - val_mse: 0.6000\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3897 - mse: 0.3681 - val_loss: 0.4033 - val_mse: 0.4033\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3727 - mse: 0.3526 - val_loss: 0.3720 - val_mse: 0.3720\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3641 - mse: 0.3448 - val_loss: 0.3659 - val_mse: 0.3659\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3580 - mse: 0.3391 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3530 - mse: 0.3345 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3494 - mse: 0.3312 - val_loss: 0.3619 - val_mse: 0.3619\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3462 - mse: 0.3282 - val_loss: 0.3613 - val_mse: 0.3613\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3437 - mse: 0.3259 - val_loss: 0.3632 - val_mse: 0.3632\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3410 - mse: 0.3234 - val_loss: 0.3635 - val_mse: 0.3635\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3390 - mse: 0.3216 - val_loss: 0.3647 - val_mse: 0.3647\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3365 - mse: 0.3192 - val_loss: 0.3675 - val_mse: 0.3675\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3343 - mse: 0.3172 - val_loss: 0.3675 - val_mse: 0.3675\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3330 - mse: 0.3159 - val_loss: 0.3668 - val_mse: 0.3668\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3305 - mse: 0.3137 - val_loss: 0.3660 - val_mse: 0.3660\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3195 - mse: 0.3035 - val_loss: 0.3647 - val_mse: 0.3647\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3151 - mse: 0.2994 - val_loss: 0.3664 - val_mse: 0.3664\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.3134 - mse: 0.2979 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5068 - mse: 0.4775 - val_loss: 0.4324 - val_mse: 0.4324\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3766 - mse: 0.3570 - val_loss: 0.3678 - val_mse: 0.3678\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3573 - mse: 0.3397 - val_loss: 0.3570 - val_mse: 0.3570\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3477 - mse: 0.3311 - val_loss: 0.3544 - val_mse: 0.3544\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3414 - mse: 0.3253 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3369 - mse: 0.3211 - val_loss: 0.3514 - val_mse: 0.3514\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3336 - mse: 0.3180 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3301 - mse: 0.3148 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3273 - mse: 0.3121 - val_loss: 0.3537 - val_mse: 0.3537\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3246 - mse: 0.3096 - val_loss: 0.3524 - val_mse: 0.3524\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3225 - mse: 0.3077 - val_loss: 0.3544 - val_mse: 0.3544\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3203 - mse: 0.3056 - val_loss: 0.3558 - val_mse: 0.3558\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3183 - mse: 0.3037 - val_loss: 0.3569 - val_mse: 0.3569\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3169 - mse: 0.3023 - val_loss: 0.3583 - val_mse: 0.3583\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3051 - mse: 0.2914 - val_loss: 0.3552 - val_mse: 0.3552\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3012 - mse: 0.2878 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.2998 - mse: 0.2864 - val_loss: 0.3578 - val_mse: 0.3578\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.4652 - mse: 0.4382 - val_loss: 0.3955 - val_mse: 0.3955\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3741 - mse: 0.3533 - val_loss: 0.3698 - val_mse: 0.3698\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3620 - mse: 0.3421 - val_loss: 0.3658 - val_mse: 0.3658\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3553 - mse: 0.3359 - val_loss: 0.3654 - val_mse: 0.3654\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3502 - mse: 0.3313 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3464 - mse: 0.3278 - val_loss: 0.3662 - val_mse: 0.3662\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3432 - mse: 0.3247 - val_loss: 0.3660 - val_mse: 0.3660\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3404 - mse: 0.3222 - val_loss: 0.3695 - val_mse: 0.3695\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3373 - mse: 0.3193 - val_loss: 0.3679 - val_mse: 0.3679\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3349 - mse: 0.3171 - val_loss: 0.3713 - val_mse: 0.3713\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3324 - mse: 0.3148 - val_loss: 0.3718 - val_mse: 0.3718\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3219 - mse: 0.3051 - val_loss: 0.3706 - val_mse: 0.3706\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3181 - mse: 0.3016 - val_loss: 0.3719 - val_mse: 0.3719\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3164 - mse: 0.3000 - val_loss: 0.3731 - val_mse: 0.3731\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5658 - mse: 0.5339 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3889 - mse: 0.3685 - val_loss: 0.3863 - val_mse: 0.3863\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3691 - mse: 0.3503 - val_loss: 0.3748 - val_mse: 0.3748\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3593 - mse: 0.3414 - val_loss: 0.3681 - val_mse: 0.3681\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3529 - mse: 0.3354 - val_loss: 0.3684 - val_mse: 0.3684\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3483 - mse: 0.3311 - val_loss: 0.3679 - val_mse: 0.3679\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3438 - mse: 0.3270 - val_loss: 0.3673 - val_mse: 0.3673\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3410 - mse: 0.3243 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3378 - mse: 0.3214 - val_loss: 0.3673 - val_mse: 0.3673\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3353 - mse: 0.3191 - val_loss: 0.3687 - val_mse: 0.3687\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3329 - mse: 0.3168 - val_loss: 0.3698 - val_mse: 0.3698\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3312 - mse: 0.3153 - val_loss: 0.3725 - val_mse: 0.3725\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3290 - mse: 0.3132 - val_loss: 0.3715 - val_mse: 0.3715\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3262 - mse: 0.3106 - val_loss: 0.3745 - val_mse: 0.3745\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3240 - mse: 0.3085 - val_loss: 0.3731 - val_mse: 0.3731\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3122 - mse: 0.2976 - val_loss: 0.3700 - val_mse: 0.3700\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3079 - mse: 0.2936 - val_loss: 0.3713 - val_mse: 0.3713\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.3062 - mse: 0.2921 - val_loss: 0.3720 - val_mse: 0.3720\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.6542 - mse: 0.6172 - val_loss: 0.5487 - val_mse: 0.5487\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4238 - mse: 0.4010 - val_loss: 0.4416 - val_mse: 0.4416\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.4050 - mse: 0.3836 - val_loss: 0.4244 - val_mse: 0.4244\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3944 - mse: 0.3738 - val_loss: 0.4095 - val_mse: 0.4095\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3873 - mse: 0.3672 - val_loss: 0.4023 - val_mse: 0.4023\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3818 - mse: 0.3621 - val_loss: 0.4048 - val_mse: 0.4048\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3769 - mse: 0.3576 - val_loss: 0.4028 - val_mse: 0.4028\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3731 - mse: 0.3540 - val_loss: 0.4043 - val_mse: 0.4043\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3696 - mse: 0.3508 - val_loss: 0.4014 - val_mse: 0.4014\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3666 - mse: 0.3480 - val_loss: 0.4051 - val_mse: 0.4051\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3642 - mse: 0.3458 - val_loss: 0.4051 - val_mse: 0.4051\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3613 - mse: 0.3430 - val_loss: 0.4064 - val_mse: 0.4064\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3587 - mse: 0.3407 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3563 - mse: 0.3384 - val_loss: 0.4084 - val_mse: 0.4084\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3540 - mse: 0.3363 - val_loss: 0.4114 - val_mse: 0.4114\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3524 - mse: 0.3347 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3385 - mse: 0.3219 - val_loss: 0.4052 - val_mse: 0.4052\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.3337 - mse: 0.3175 - val_loss: 0.4059 - val_mse: 0.4059\n",
      "Epoch 19/2000\n",
      " - 9s - loss: 0.3317 - mse: 0.3156 - val_loss: 0.4077 - val_mse: 0.4077\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5883 - mse: 0.5548 - val_loss: 0.4674 - val_mse: 0.4674\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4324 - mse: 0.4092 - val_loss: 0.4266 - val_mse: 0.4266\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.4140 - mse: 0.3922 - val_loss: 0.4146 - val_mse: 0.4146\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.4027 - mse: 0.3816 - val_loss: 0.4095 - val_mse: 0.4095\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3949 - mse: 0.3744 - val_loss: 0.4109 - val_mse: 0.4109\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3887 - mse: 0.3687 - val_loss: 0.4114 - val_mse: 0.4114\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3837 - mse: 0.3640 - val_loss: 0.4143 - val_mse: 0.4143\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3794 - mse: 0.3601 - val_loss: 0.4149 - val_mse: 0.4149\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3757 - mse: 0.3566 - val_loss: 0.4113 - val_mse: 0.4113\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3721 - mse: 0.3533 - val_loss: 0.4185 - val_mse: 0.4185\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3690 - mse: 0.3504 - val_loss: 0.4175 - val_mse: 0.4175\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3560 - mse: 0.3383 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3518 - mse: 0.3345 - val_loss: 0.4150 - val_mse: 0.4150\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3501 - mse: 0.3329 - val_loss: 0.4165 - val_mse: 0.4165\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5514 - mse: 0.5197 - val_loss: 0.4611 - val_mse: 0.4611\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4135 - mse: 0.3901 - val_loss: 0.4199 - val_mse: 0.4199\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3975 - mse: 0.3752 - val_loss: 0.4087 - val_mse: 0.4087\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3888 - mse: 0.3672 - val_loss: 0.4054 - val_mse: 0.4054\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3821 - mse: 0.3609 - val_loss: 0.4051 - val_mse: 0.4051\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3774 - mse: 0.3565 - val_loss: 0.4071 - val_mse: 0.4071\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3735 - mse: 0.3529 - val_loss: 0.4059 - val_mse: 0.4059\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3697 - mse: 0.3495 - val_loss: 0.4064 - val_mse: 0.4064\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3667 - mse: 0.3467 - val_loss: 0.4098 - val_mse: 0.4098\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3638 - mse: 0.3440 - val_loss: 0.4052 - val_mse: 0.4052\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3615 - mse: 0.3418 - val_loss: 0.4080 - val_mse: 0.4080\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3585 - mse: 0.3390 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3463 - mse: 0.3278 - val_loss: 0.4090 - val_mse: 0.4090\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3422 - mse: 0.3240 - val_loss: 0.4102 - val_mse: 0.4102\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3406 - mse: 0.3225 - val_loss: 0.4106 - val_mse: 0.4106\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.6143 - mse: 0.5784 - val_loss: 0.4423 - val_mse: 0.4423\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4108 - mse: 0.3880 - val_loss: 0.4025 - val_mse: 0.4025\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3915 - mse: 0.3701 - val_loss: 0.3980 - val_mse: 0.3980\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3816 - mse: 0.3610 - val_loss: 0.3956 - val_mse: 0.3956\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3752 - mse: 0.3550 - val_loss: 0.3933 - val_mse: 0.3933\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3706 - mse: 0.3508 - val_loss: 0.3925 - val_mse: 0.3925\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3664 - mse: 0.3468 - val_loss: 0.3897 - val_mse: 0.3897\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3631 - mse: 0.3438 - val_loss: 0.3909 - val_mse: 0.3909\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3597 - mse: 0.3406 - val_loss: 0.3911 - val_mse: 0.3911\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3571 - mse: 0.3382 - val_loss: 0.3902 - val_mse: 0.3902\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3549 - mse: 0.3362 - val_loss: 0.3937 - val_mse: 0.3937\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3525 - mse: 0.3339 - val_loss: 0.3915 - val_mse: 0.3915\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3501 - mse: 0.3317 - val_loss: 0.3931 - val_mse: 0.3931\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3481 - mse: 0.3299 - val_loss: 0.3966 - val_mse: 0.3966\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3368 - mse: 0.3195 - val_loss: 0.3918 - val_mse: 0.3918\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3328 - mse: 0.3157 - val_loss: 0.3926 - val_mse: 0.3926\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.3312 - mse: 0.3143 - val_loss: 0.3938 - val_mse: 0.3938\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2000\n",
    "\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * num_days) * 0.25 + 1 )\n",
    "for i in range(15):\n",
    "    print(\"=\" * 50)\n",
    "    print(f'Step {i+1}')\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    y_mean = y.mean()\n",
    "    xv = X_val\n",
    "    yv = y_val[:, i]\n",
    "    model = build_model()\n",
    "\n",
    "    opt = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "        ]\n",
    "    \n",
    "    #smaller batch size runs faster\n",
    "    #batch_size = 65536\n",
    "    batch_size = 8192\n",
    "\n",
    "    model.fit(X_train, y - y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv-y_mean), callbacks=callbacks )\n",
    "    val_pred.append(model.predict(X_val) + y_mean)\n",
    "    test_pred.append(model.predict(X_test) + y_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation nwrmsle = 0.6098121112836306\n"
     ]
    }
   ],
   "source": [
    "weight = items[\"perishable\"] * 0.25 + 1\n",
    "val_err = (y_val - np.array(val_pred).squeeze(axis=2).transpose())**2\n",
    "val_err = val_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "val_err = np.sqrt(val_err.sum() / weight.sum() / 15)\n",
    "print(f'validation nwrmsle = {val_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nwrmsle = 0.622227862622076\n"
     ]
    }
   ],
   "source": [
    "test_err = (y_test - np.array(test_pred).squeeze(axis=2).transpose())**2\n",
    "test_err = test_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "test_err = np.sqrt(test_err.sum() / weight.sum() / 15)\n",
    "print(f'test nwrmsle = {test_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test = pd.DataFrame(np.array(test_pred).squeeze(axis=2).transpose(), \n",
    "                            index = df_2017_index, columns = pd.date_range('2017-08-01',periods=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../model_results/2020-01-08/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test.to_csv(out_path + 'nn_test_pred_model_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
