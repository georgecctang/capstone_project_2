{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Dates\n",
    "\n",
    "Num Training Weeks = 4  \n",
    "Num Test/Val Days = 15\n",
    "\n",
    "- Train: 5/30/2017 (Tues), 6/6/2017,6/13/2017,6/20/2017  \n",
    "- Val: 7/11/2017 (Tues) - 7/25/2017 (Wed)  \n",
    "- Test: 8/1/2017 (Tues)  - 8/15/2017 (Wed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sales data from 2017-01-01 and beyond\n",
    "\n",
    "df_2017 = pd.read_csv(\n",
    "    '../input/train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 101688780))\n",
    "\n",
    "\n",
    "# import items and stores data\n",
    "items = pd.read_csv(\"../input/items.csv\").set_index(\"item_nbr\")\n",
    "stores = pd.read_csv(\"../input/stores.csv\").set_index(\"store_nbr\")\n",
    "\n",
    "# Create promotion dataset\n",
    "promo_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017.columns = promo_2017.columns.get_level_values(1)\n",
    "\n",
    "# Transform sales training data\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "df_2017_index = df_2017.index\n",
    "\n",
    "# transform items dataset\n",
    "items['class'] = items['class'].astype('category')\n",
    "items = pd.get_dummies(items)\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "# transform \n",
    "stores['cluster'] = stores.cluster.astype('category')\n",
    "stores = pd.get_dummies(stores)\n",
    "stores = stores.reindex(df_2017.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(df, promo_df, t2017, is_train=True, name_prefix=None):\n",
    "    X = {\n",
    "        \n",
    "        # Number of promotion in the last x days \n",
    "        \"promo_7_2017\": get_timespan(promo_df, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_df, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_30_2017\": get_timespan(promo_df, t2017, 30, 30).sum(axis=1).values,\n",
    "        \n",
    "        # Number of promotion in the next x days of reference date\n",
    "        \"promo_3_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 3).sum(axis=1).values,\n",
    "        \"promo_7_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14).sum(axis=1).values,\n",
    "    }\n",
    "    \n",
    "# Removed due to the presence of nan values \n",
    "#     for i in [3, 7, 14, 30]:\n",
    "#         tmp1 = get_timespan(df, t2017, i, i)\n",
    "#         tmp2 = (get_timespan(promo_df, t2017, i, i) > 0) * 1\n",
    "\n",
    "#         X['has_promo_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n",
    "#         X['no_promo_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).values\n",
    "    \n",
    "    for i in [3, 7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # mean daily difference in sales in the last x days\n",
    "        X[f'diff_{i}_mean'] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        # mean sales in the last x days\n",
    "        X[f'mean_{i}'] = tmp.mean(axis=1).values\n",
    "        # median sales in the last x days\n",
    "        X[f'median_{i}'] = tmp.median(axis=1).values\n",
    "        # min sales in the last x days\n",
    "        X[f'min_{i}'] = tmp.min(axis=1).values\n",
    "        # max sales in the last x days\n",
    "        X[f'max_{i}'] = tmp.max(axis=1).values\n",
    "        # std dev sales in the last x days\n",
    "        X[f'std_{i}'] = tmp.std(axis=1).values\n",
    "\n",
    "    for i in [7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # number of days with sales in the last x days\n",
    "        X[f'has_sales_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day with sales in the last x days\n",
    "        X[f'last_has_sales_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first of days with sales in the last x days\n",
    "        X[f'first_has_sales_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "        tmp = get_timespan(promo_df, t2017, i, i)\n",
    "        # number of days with promotions in the last x days\n",
    "        X[f'has_promo_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day has promotion in the last x days\n",
    "        X[f'last_has_promo_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first day has promotion in the last x days\n",
    "        X[f'first_has_promo_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "    tmp = get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14)\n",
    "    # last day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    # Count backwards: if last day is 8/15, then value = 1\n",
    "    # if last day is 8/2, then value = 14\n",
    "    X['last_has_promo_day_in_after_14_days'] = 14 - ((tmp > 0) * np.arange(14)).max(axis=1).values\n",
    "    # first day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    X['first_has_promo_day_in_after_14_days'] = ((tmp > 0) * np.arange(14, 0, -1)).max(axis=1).values\n",
    "\n",
    "    # sale on day x days from reference date \n",
    "    for i in range(1, 15):\n",
    "        X[f'day_{i}_2017'] = get_timespan(df, t2017, i, 1).values.ravel()\n",
    "    \n",
    "    # average sales on day of the week for the last 4 or 20 weeks\n",
    "    for i in range(7):\n",
    "        X[f'mean_4_dow{i}_2017'] = get_timespan(df, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X[f'mean_20_dow{i}_2017'] = get_timespan(df, t2017, 140-i, 20, freq='7D').mean(axis=1).values        \n",
    "    \n",
    "    # promotion status of each day 14 days before and 14 days after the reference date\n",
    "    \n",
    "    for i in range(-14, 15):\n",
    "        X[f'promo_{i}'] = promo_df[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    if name_prefix is not None:\n",
    "        X.columns = [f'{name_prefix}_{c}' for c in X.columns]\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(t2017, periods=15)].values\n",
    "        return X, y\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create training data\n",
    "\n",
    "t2017 = date(2017, 5, 30)\n",
    "num_days = 4\n",
    "X_l, y_l = [], []\n",
    "for i in tqdm(range(num_days)):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(df_2017, promo_2017, t2017 + delta)\n",
    "\n",
    "    X_tmp = pd.concat([X_tmp, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "\n",
    "del X_l, y_l\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data\n",
    "\n",
    "X_val, y_val = prepare_dataset(df_2017, promo_2017, date(2017, 7, 11))\n",
    "\n",
    "X_val = pd.concat([X_val, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_test, y_test = prepare_dataset(df_2017, promo_2017, date(2017, 8, 1))\n",
    "\n",
    "X_test = pd.concat([X_test, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670060, 538) (670060, 15)\n",
      "(167515, 538) (167515, 15)\n",
      "(167515, 538) (167515, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_2017, promo_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_test[:] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "X_val = X_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512 x 256 x 128 x 64 x 32 x 16 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.4198 - mse: 0.3952 - val_loss: 0.3758 - val_mse: 0.3758\n",
      "Epoch 2/2000\n",
      " - 8s - loss: 0.3378 - mse: 0.3191 - val_loss: 0.3284 - val_mse: 0.3284\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3271 - mse: 0.3093 - val_loss: 0.3195 - val_mse: 0.3195\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3204 - mse: 0.3031 - val_loss: 0.3164 - val_mse: 0.3164\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3157 - mse: 0.2988 - val_loss: 0.3161 - val_mse: 0.3161\n",
      "Epoch 6/2000\n",
      " - 8s - loss: 0.3117 - mse: 0.2951 - val_loss: 0.3154 - val_mse: 0.3154\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3084 - mse: 0.2920 - val_loss: 0.3181 - val_mse: 0.3181\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3052 - mse: 0.2891 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3025 - mse: 0.2865 - val_loss: 0.3206 - val_mse: 0.3206\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.2989 - mse: 0.2832 - val_loss: 0.3189 - val_mse: 0.3189\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.2965 - mse: 0.2810 - val_loss: 0.3227 - val_mse: 0.3227\n",
      "Epoch 12/2000\n",
      " - 8s - loss: 0.2933 - mse: 0.2780 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2903 - mse: 0.2752 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2731 - mse: 0.2594 - val_loss: 0.3235 - val_mse: 0.3235\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2645 - mse: 0.2514 - val_loss: 0.3266 - val_mse: 0.3266\n",
      "Epoch 16/2000\n",
      " - 8s - loss: 0.2602 - mse: 0.2475 - val_loss: 0.3287 - val_mse: 0.3287\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.4988 - mse: 0.4703 - val_loss: 0.4420 - val_mse: 0.4420\n",
      "Epoch 2/2000\n",
      " - 8s - loss: 0.3496 - mse: 0.3316 - val_loss: 0.3491 - val_mse: 0.3491\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3337 - mse: 0.3171 - val_loss: 0.3252 - val_mse: 0.3252\n",
      "Epoch 4/2000\n",
      " - 8s - loss: 0.3246 - mse: 0.3088 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3184 - mse: 0.3031 - val_loss: 0.3184 - val_mse: 0.3184\n",
      "Epoch 6/2000\n",
      " - 8s - loss: 0.3139 - mse: 0.2988 - val_loss: 0.3193 - val_mse: 0.3193\n",
      "Epoch 7/2000\n",
      " - 8s - loss: 0.3098 - mse: 0.2950 - val_loss: 0.3185 - val_mse: 0.3185\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3061 - mse: 0.2915 - val_loss: 0.3189 - val_mse: 0.3189\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3026 - mse: 0.2883 - val_loss: 0.3230 - val_mse: 0.3230\n",
      "Epoch 10/2000\n",
      " - 8s - loss: 0.3000 - mse: 0.2858 - val_loss: 0.3240 - val_mse: 0.3240\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.2969 - mse: 0.2829 - val_loss: 0.3244 - val_mse: 0.3244\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2946 - mse: 0.2807 - val_loss: 0.3285 - val_mse: 0.3285\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 8s - loss: 0.2780 - mse: 0.2653 - val_loss: 0.3224 - val_mse: 0.3224\n",
      "Epoch 14/2000\n",
      " - 8s - loss: 0.2713 - mse: 0.2590 - val_loss: 0.3238 - val_mse: 0.3238\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2679 - mse: 0.2558 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.4566 - mse: 0.4305 - val_loss: 0.3871 - val_mse: 0.3871\n",
      "Epoch 2/2000\n",
      " - 8s - loss: 0.3660 - mse: 0.3459 - val_loss: 0.3517 - val_mse: 0.3517\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3552 - mse: 0.3357 - val_loss: 0.3432 - val_mse: 0.3432\n",
      "Epoch 4/2000\n",
      " - 8s - loss: 0.3480 - mse: 0.3290 - val_loss: 0.3433 - val_mse: 0.3433\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3422 - mse: 0.3237 - val_loss: 0.3431 - val_mse: 0.3431\n",
      "Epoch 6/2000\n",
      " - 8s - loss: 0.3380 - mse: 0.3197 - val_loss: 0.3479 - val_mse: 0.3479\n",
      "Epoch 7/2000\n",
      " - 8s - loss: 0.3341 - mse: 0.3161 - val_loss: 0.3436 - val_mse: 0.3436\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3300 - mse: 0.3123 - val_loss: 0.3512 - val_mse: 0.3512\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3269 - mse: 0.3095 - val_loss: 0.3478 - val_mse: 0.3478\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3234 - mse: 0.3062 - val_loss: 0.3488 - val_mse: 0.3488\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3199 - mse: 0.3029 - val_loss: 0.3503 - val_mse: 0.3503\n",
      "Epoch 12/2000\n",
      " - 8s - loss: 0.3169 - mse: 0.3001 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2987 - mse: 0.2833 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 14/2000\n",
      " - 8s - loss: 0.2903 - mse: 0.2755 - val_loss: 0.3542 - val_mse: 0.3542\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2862 - mse: 0.2717 - val_loss: 0.3566 - val_mse: 0.3566\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.4675 - mse: 0.4414 - val_loss: 0.3954 - val_mse: 0.3954\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3654 - mse: 0.3466 - val_loss: 0.3541 - val_mse: 0.3541\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3515 - mse: 0.3338 - val_loss: 0.3467 - val_mse: 0.3467\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3435 - mse: 0.3263 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3377 - mse: 0.3209 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3329 - mse: 0.3165 - val_loss: 0.3524 - val_mse: 0.3524\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3286 - mse: 0.3125 - val_loss: 0.3467 - val_mse: 0.3467\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3252 - mse: 0.3091 - val_loss: 0.3499 - val_mse: 0.3499\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3213 - mse: 0.3056 - val_loss: 0.3500 - val_mse: 0.3500\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3182 - mse: 0.3027 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3146 - mse: 0.2993 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2973 - mse: 0.2832 - val_loss: 0.3477 - val_mse: 0.3477\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2892 - mse: 0.2757 - val_loss: 0.3501 - val_mse: 0.3501\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2855 - mse: 0.2723 - val_loss: 0.3521 - val_mse: 0.3521\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5272 - mse: 0.4980 - val_loss: 0.4442 - val_mse: 0.4442\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3922 - mse: 0.3711 - val_loss: 0.3791 - val_mse: 0.3791\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3768 - mse: 0.3567 - val_loss: 0.3711 - val_mse: 0.3711\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3676 - mse: 0.3481 - val_loss: 0.3655 - val_mse: 0.3655\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3605 - mse: 0.3415 - val_loss: 0.3691 - val_mse: 0.3691\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3550 - mse: 0.3364 - val_loss: 0.3695 - val_mse: 0.3695\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3500 - mse: 0.3317 - val_loss: 0.3678 - val_mse: 0.3678\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3461 - mse: 0.3280 - val_loss: 0.3782 - val_mse: 0.3782\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3423 - mse: 0.3245 - val_loss: 0.3725 - val_mse: 0.3725\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3387 - mse: 0.3212 - val_loss: 0.3800 - val_mse: 0.3800\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3345 - mse: 0.3172 - val_loss: 0.3792 - val_mse: 0.3792\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3159 - mse: 0.3000 - val_loss: 0.3726 - val_mse: 0.3726\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3079 - mse: 0.2926 - val_loss: 0.3743 - val_mse: 0.3743\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3042 - mse: 0.2891 - val_loss: 0.3771 - val_mse: 0.3771\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5859 - mse: 0.5522 - val_loss: 0.4348 - val_mse: 0.4348\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4050 - mse: 0.3831 - val_loss: 0.3967 - val_mse: 0.3967\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3866 - mse: 0.3660 - val_loss: 0.3883 - val_mse: 0.3883\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3754 - mse: 0.3556 - val_loss: 0.3807 - val_mse: 0.3807\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3675 - mse: 0.3483 - val_loss: 0.3825 - val_mse: 0.3825\n",
      "Epoch 6/2000\n",
      " - 8s - loss: 0.3611 - mse: 0.3424 - val_loss: 0.3830 - val_mse: 0.3830\n",
      "Epoch 7/2000\n",
      " - 8s - loss: 0.3559 - mse: 0.3375 - val_loss: 0.3824 - val_mse: 0.3824\n",
      "Epoch 8/2000\n",
      " - 8s - loss: 0.3513 - mse: 0.3333 - val_loss: 0.3821 - val_mse: 0.3821\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3471 - mse: 0.3293 - val_loss: 0.3885 - val_mse: 0.3885\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3430 - mse: 0.3256 - val_loss: 0.3892 - val_mse: 0.3892\n",
      "Epoch 11/2000\n",
      " - 8s - loss: 0.3394 - mse: 0.3222 - val_loss: 0.3915 - val_mse: 0.3915\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 8s - loss: 0.3193 - mse: 0.3037 - val_loss: 0.3864 - val_mse: 0.3864\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3115 - mse: 0.2964 - val_loss: 0.3879 - val_mse: 0.3879\n",
      "Epoch 14/2000\n",
      " - 8s - loss: 0.3078 - mse: 0.2930 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.4801 - mse: 0.4521 - val_loss: 0.4183 - val_mse: 0.4183\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3871 - mse: 0.3650 - val_loss: 0.3814 - val_mse: 0.3814\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3746 - mse: 0.3534 - val_loss: 0.3689 - val_mse: 0.3689\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3671 - mse: 0.3465 - val_loss: 0.3700 - val_mse: 0.3700\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3613 - mse: 0.3411 - val_loss: 0.3704 - val_mse: 0.3704\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3565 - mse: 0.3367 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3520 - mse: 0.3325 - val_loss: 0.3713 - val_mse: 0.3713\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3483 - mse: 0.3291 - val_loss: 0.3736 - val_mse: 0.3736\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3441 - mse: 0.3252 - val_loss: 0.3742 - val_mse: 0.3742\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3406 - mse: 0.3219 - val_loss: 0.3755 - val_mse: 0.3755\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3237 - mse: 0.3063 - val_loss: 0.3756 - val_mse: 0.3756\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3160 - mse: 0.2992 - val_loss: 0.3770 - val_mse: 0.3770\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3121 - mse: 0.2956 - val_loss: 0.3788 - val_mse: 0.3788\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5381 - mse: 0.5074 - val_loss: 0.4436 - val_mse: 0.4436\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3817 - mse: 0.3608 - val_loss: 0.3835 - val_mse: 0.3835\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3652 - mse: 0.3456 - val_loss: 0.3678 - val_mse: 0.3678\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3558 - mse: 0.3370 - val_loss: 0.3642 - val_mse: 0.3642\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3490 - mse: 0.3307 - val_loss: 0.3649 - val_mse: 0.3649\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3440 - mse: 0.3261 - val_loss: 0.3641 - val_mse: 0.3641\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3392 - mse: 0.3216 - val_loss: 0.3662 - val_mse: 0.3662\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3351 - mse: 0.3178 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3316 - mse: 0.3146 - val_loss: 0.3702 - val_mse: 0.3702\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3283 - mse: 0.3115 - val_loss: 0.3717 - val_mse: 0.3717\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3255 - mse: 0.3088 - val_loss: 0.3752 - val_mse: 0.3752\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3216 - mse: 0.3052 - val_loss: 0.3725 - val_mse: 0.3725\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3189 - mse: 0.3026 - val_loss: 0.3790 - val_mse: 0.3790\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2997 - mse: 0.2849 - val_loss: 0.3733 - val_mse: 0.3733\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2911 - mse: 0.2769 - val_loss: 0.3748 - val_mse: 0.3748\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.2871 - mse: 0.2732 - val_loss: 0.3774 - val_mse: 0.3774\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.4909 - mse: 0.4635 - val_loss: 0.4122 - val_mse: 0.4122\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3659 - mse: 0.3475 - val_loss: 0.3624 - val_mse: 0.3624\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3491 - mse: 0.3323 - val_loss: 0.3504 - val_mse: 0.3504\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3406 - mse: 0.3246 - val_loss: 0.3496 - val_mse: 0.3496\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3343 - mse: 0.3187 - val_loss: 0.3506 - val_mse: 0.3506\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3300 - mse: 0.3147 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3253 - mse: 0.3103 - val_loss: 0.3507 - val_mse: 0.3507\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3215 - mse: 0.3067 - val_loss: 0.3548 - val_mse: 0.3548\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3177 - mse: 0.3032 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3145 - mse: 0.3001 - val_loss: 0.3559 - val_mse: 0.3559\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3108 - mse: 0.2966 - val_loss: 0.3591 - val_mse: 0.3591\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2956 - mse: 0.2825 - val_loss: 0.3547 - val_mse: 0.3547\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2886 - mse: 0.2759 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2851 - mse: 0.2727 - val_loss: 0.3582 - val_mse: 0.3582\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5150 - mse: 0.4852 - val_loss: 0.4205 - val_mse: 0.4205\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3732 - mse: 0.3527 - val_loss: 0.3758 - val_mse: 0.3758\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3588 - mse: 0.3393 - val_loss: 0.3677 - val_mse: 0.3677\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3506 - mse: 0.3316 - val_loss: 0.3688 - val_mse: 0.3688\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3443 - mse: 0.3258 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3394 - mse: 0.3212 - val_loss: 0.3692 - val_mse: 0.3692\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3350 - mse: 0.3172 - val_loss: 0.3736 - val_mse: 0.3736\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3311 - mse: 0.3136 - val_loss: 0.3731 - val_mse: 0.3731\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3274 - mse: 0.3101 - val_loss: 0.3738 - val_mse: 0.3738\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3239 - mse: 0.3068 - val_loss: 0.3757 - val_mse: 0.3757\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3072 - mse: 0.2914 - val_loss: 0.3780 - val_mse: 0.3780\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3003 - mse: 0.2851 - val_loss: 0.3801 - val_mse: 0.3801\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2971 - mse: 0.2821 - val_loss: 0.3805 - val_mse: 0.3805\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5125 - mse: 0.4837 - val_loss: 0.4213 - val_mse: 0.4213\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3770 - mse: 0.3576 - val_loss: 0.3768 - val_mse: 0.3768\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3608 - mse: 0.3427 - val_loss: 0.3675 - val_mse: 0.3675\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3517 - mse: 0.3344 - val_loss: 0.3642 - val_mse: 0.3642\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3453 - mse: 0.3284 - val_loss: 0.3652 - val_mse: 0.3652\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3400 - mse: 0.3234 - val_loss: 0.3650 - val_mse: 0.3650\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3355 - mse: 0.3192 - val_loss: 0.3653 - val_mse: 0.3653\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3317 - mse: 0.3157 - val_loss: 0.3691 - val_mse: 0.3691\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3281 - mse: 0.3123 - val_loss: 0.3702 - val_mse: 0.3702\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3242 - mse: 0.3087 - val_loss: 0.3693 - val_mse: 0.3693\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3208 - mse: 0.3055 - val_loss: 0.3712 - val_mse: 0.3712\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3034 - mse: 0.2894 - val_loss: 0.3708 - val_mse: 0.3708\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2960 - mse: 0.2824 - val_loss: 0.3727 - val_mse: 0.3727\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2926 - mse: 0.2793 - val_loss: 0.3734 - val_mse: 0.3734\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 10s - loss: 0.5203 - mse: 0.4913 - val_loss: 0.4585 - val_mse: 0.4585\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4093 - mse: 0.3875 - val_loss: 0.4083 - val_mse: 0.4083\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3945 - mse: 0.3738 - val_loss: 0.3972 - val_mse: 0.3972\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3851 - mse: 0.3651 - val_loss: 0.3983 - val_mse: 0.3983\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3782 - mse: 0.3586 - val_loss: 0.3967 - val_mse: 0.3967\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3724 - mse: 0.3532 - val_loss: 0.3992 - val_mse: 0.3992\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3671 - mse: 0.3484 - val_loss: 0.3984 - val_mse: 0.3984\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3626 - mse: 0.3441 - val_loss: 0.4066 - val_mse: 0.4066\n",
      "Epoch 9/2000\n",
      " - 8s - loss: 0.3583 - mse: 0.3401 - val_loss: 0.4033 - val_mse: 0.4033\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3541 - mse: 0.3362 - val_loss: 0.4128 - val_mse: 0.4128\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3502 - mse: 0.3326 - val_loss: 0.4224 - val_mse: 0.4224\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3459 - mse: 0.3286 - val_loss: 0.4145 - val_mse: 0.4145\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3257 - mse: 0.3098 - val_loss: 0.4054 - val_mse: 0.4054\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3161 - mse: 0.3010 - val_loss: 0.4077 - val_mse: 0.4077\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3116 - mse: 0.2968 - val_loss: 0.4122 - val_mse: 0.4122\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.6136 - mse: 0.5801 - val_loss: 0.4762 - val_mse: 0.4762\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4297 - mse: 0.4069 - val_loss: 0.4272 - val_mse: 0.4272\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.4091 - mse: 0.3878 - val_loss: 0.4166 - val_mse: 0.4166\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3970 - mse: 0.3765 - val_loss: 0.4143 - val_mse: 0.4143\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3886 - mse: 0.3686 - val_loss: 0.4171 - val_mse: 0.4171\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3810 - mse: 0.3616 - val_loss: 0.4195 - val_mse: 0.4195\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3751 - mse: 0.3560 - val_loss: 0.4176 - val_mse: 0.4176\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3699 - mse: 0.3511 - val_loss: 0.4191 - val_mse: 0.4191\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3652 - mse: 0.3467 - val_loss: 0.4254 - val_mse: 0.4254\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3604 - mse: 0.3423 - val_loss: 0.4255 - val_mse: 0.4255\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3562 - mse: 0.3383 - val_loss: 0.4275 - val_mse: 0.4275\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3362 - mse: 0.3198 - val_loss: 0.4221 - val_mse: 0.4221\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3275 - mse: 0.3117 - val_loss: 0.4246 - val_mse: 0.4246\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3237 - mse: 0.3082 - val_loss: 0.4255 - val_mse: 0.4255\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5057 - mse: 0.4763 - val_loss: 0.4285 - val_mse: 0.4285\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4040 - mse: 0.3812 - val_loss: 0.4044 - val_mse: 0.4044\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3899 - mse: 0.3681 - val_loss: 0.4009 - val_mse: 0.4009\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3813 - mse: 0.3602 - val_loss: 0.4027 - val_mse: 0.4027\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3747 - mse: 0.3541 - val_loss: 0.4032 - val_mse: 0.4032\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3702 - mse: 0.3498 - val_loss: 0.4066 - val_mse: 0.4066\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3653 - mse: 0.3454 - val_loss: 0.4081 - val_mse: 0.4081\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3613 - mse: 0.3416 - val_loss: 0.4096 - val_mse: 0.4096\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3567 - mse: 0.3374 - val_loss: 0.4117 - val_mse: 0.4117\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3536 - mse: 0.3344 - val_loss: 0.4118 - val_mse: 0.4118\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3353 - mse: 0.3175 - val_loss: 0.4113 - val_mse: 0.4113\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3271 - mse: 0.3100 - val_loss: 0.4149 - val_mse: 0.4149\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3230 - mse: 0.3062 - val_loss: 0.4165 - val_mse: 0.4165\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5122 - mse: 0.4829 - val_loss: 0.4366 - val_mse: 0.4366\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3921 - mse: 0.3705 - val_loss: 0.3927 - val_mse: 0.3927\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3776 - mse: 0.3572 - val_loss: 0.3994 - val_mse: 0.3994\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3690 - mse: 0.3493 - val_loss: 0.3919 - val_mse: 0.3919\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3622 - mse: 0.3430 - val_loss: 0.3920 - val_mse: 0.3920\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3570 - mse: 0.3381 - val_loss: 0.3957 - val_mse: 0.3957\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3522 - mse: 0.3337 - val_loss: 0.3941 - val_mse: 0.3941\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3485 - mse: 0.3302 - val_loss: 0.3932 - val_mse: 0.3932\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3445 - mse: 0.3265 - val_loss: 0.3965 - val_mse: 0.3965\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3406 - mse: 0.3228 - val_loss: 0.4015 - val_mse: 0.4015\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3373 - mse: 0.3198 - val_loss: 0.3967 - val_mse: 0.3967\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3182 - mse: 0.3020 - val_loss: 0.3984 - val_mse: 0.3984\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3092 - mse: 0.2938 - val_loss: 0.4007 - val_mse: 0.4007\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3049 - mse: 0.2898 - val_loss: 0.4029 - val_mse: 0.4029\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2000\n",
    "\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * num_days) * 0.25 + 1 )\n",
    "for i in range(15):\n",
    "    print(\"=\" * 50)\n",
    "    print(f'Step {i+1}')\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    y_mean = y.mean()\n",
    "    xv = X_val\n",
    "    yv = y_val[:, i]\n",
    "    model = build_model()\n",
    "\n",
    "    opt = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "        ]\n",
    "    \n",
    "    #smaller batch size runs faster\n",
    "    #batch_size = 65536\n",
    "    batch_size = 8192\n",
    "\n",
    "    model.fit(X_train, y - y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv-y_mean), callbacks=callbacks )\n",
    "    val_pred.append(model.predict(X_val) + y_mean)\n",
    "    test_pred.append(model.predict(X_test) + y_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation nwrmsle = 0.6143938249014856\n"
     ]
    }
   ],
   "source": [
    "weight = items[\"perishable\"] * 0.25 + 1\n",
    "val_err = (y_val - np.array(val_pred).squeeze(axis=2).transpose())**2\n",
    "val_err = val_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "val_err = np.sqrt(val_err.sum() / weight.sum() / 15)\n",
    "print(f'validation nwrmsle = {val_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nwrmsle = 0.627665652750179\n"
     ]
    }
   ],
   "source": [
    "test_err = (y_test - np.array(test_pred).squeeze(axis=2).transpose())**2\n",
    "test_err = test_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "test_err = np.sqrt(test_err.sum() / weight.sum() / 15)\n",
    "print(f'test nwrmsle = {test_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test = pd.DataFrame(np.array(test_pred).squeeze(axis=2).transpose(), \n",
    "                            index = df_2017_index, columns = pd.date_range('2017-08-01',periods=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../model_results/2020-01-08/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test.to_csv(out_path + 'nn_test_pred_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
