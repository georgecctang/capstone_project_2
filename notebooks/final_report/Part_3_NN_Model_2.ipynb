{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Dates\n",
    "\n",
    "Num Training Weeks = 4  \n",
    "Num Test/Val Days = 15\n",
    "\n",
    "- Train: 5/30/2017 (Tues), 6/6/2017,6/13/2017,6/20/2017  \n",
    "- Val: 7/11/2017 (Tues) - 7/25/2017 (Wed)  \n",
    "- Test: 8/1/2017 (Tues)  - 8/15/2017 (Wed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sales data from 2017-01-01 and beyond\n",
    "\n",
    "df_2017 = pd.read_csv(\n",
    "    '../input/train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 101688780))\n",
    "\n",
    "\n",
    "# import items and stores data\n",
    "items = pd.read_csv(\"../input/items.csv\").set_index(\"item_nbr\")\n",
    "stores = pd.read_csv(\"../input/stores.csv\").set_index(\"store_nbr\")\n",
    "\n",
    "# Create promotion dataset\n",
    "promo_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017.columns = promo_2017.columns.get_level_values(1)\n",
    "\n",
    "# Transform sales training data\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "df_2017_index = df_2017.index\n",
    "\n",
    "# transform items dataset\n",
    "items['class'] = items['class'].astype('category')\n",
    "items = pd.get_dummies(items)\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "# transform \n",
    "stores['cluster'] = stores.cluster.astype('category')\n",
    "stores = pd.get_dummies(stores)\n",
    "stores = stores.reindex(df_2017.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(df, promo_df, t2017, is_train=True, name_prefix=None):\n",
    "    X = {\n",
    "        \n",
    "        # Number of promotion in the last x days \n",
    "        \"promo_7_2017\": get_timespan(promo_df, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_df, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_30_2017\": get_timespan(promo_df, t2017, 30, 30).sum(axis=1).values,\n",
    "        \n",
    "        # Number of promotion in the next x days of reference date\n",
    "        \"promo_3_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 3).sum(axis=1).values,\n",
    "        \"promo_7_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14).sum(axis=1).values,\n",
    "    }\n",
    "    \n",
    "# Removed due to the presence of nan values \n",
    "#     for i in [3, 7, 14, 30]:\n",
    "#         tmp1 = get_timespan(df, t2017, i, i)\n",
    "#         tmp2 = (get_timespan(promo_df, t2017, i, i) > 0) * 1\n",
    "\n",
    "#         X['has_promo_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n",
    "#         X['no_promo_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).values\n",
    "    \n",
    "    for i in [3, 7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # mean daily difference in sales in the last x days\n",
    "        X[f'diff_{i}_mean'] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        # mean sales in the last x days\n",
    "        X[f'mean_{i}'] = tmp.mean(axis=1).values\n",
    "        # median sales in the last x days\n",
    "        X[f'median_{i}'] = tmp.median(axis=1).values\n",
    "        # min sales in the last x days\n",
    "        X[f'min_{i}'] = tmp.min(axis=1).values\n",
    "        # max sales in the last x days\n",
    "        X[f'max_{i}'] = tmp.max(axis=1).values\n",
    "        # std dev sales in the last x days\n",
    "        X[f'std_{i}'] = tmp.std(axis=1).values\n",
    "\n",
    "    for i in [7, 14, 30]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        # number of days with sales in the last x days\n",
    "        X[f'has_sales_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day with sales in the last x days\n",
    "        X[f'last_has_sales_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first of days with sales in the last x days\n",
    "        X[f'first_has_sales_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "        tmp = get_timespan(promo_df, t2017, i, i)\n",
    "        # number of days with promotions in the last x days\n",
    "        X[f'has_promo_days_in_last_{i}'] = (tmp > 0).sum(axis=1).values\n",
    "        # last day has promotion in the last x days\n",
    "        X[f'last_has_promo_day_in_last_{i}'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        # first day has promotion in the last x days\n",
    "        X[f'first_has_promo_day_in_last_{i}'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "    tmp = get_timespan(promo_df, t2017 + timedelta(days=15), 14, 14)\n",
    "    # last day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    # Count backwards: if last day is 8/15, then value = 1\n",
    "    # if last day is 8/2, then value = 14\n",
    "    X['last_has_promo_day_in_after_14_days'] = 14 - ((tmp > 0) * np.arange(14)).max(axis=1).values\n",
    "    # first day that has promotion in the next 14 days (8/2 to 8/15)\n",
    "    X['first_has_promo_day_in_after_14_days'] = ((tmp > 0) * np.arange(14, 0, -1)).max(axis=1).values\n",
    "\n",
    "    # sale on day x days from reference date \n",
    "    for i in range(1, 15):\n",
    "        X[f'day_{i}_2017'] = get_timespan(df, t2017, i, 1).values.ravel()\n",
    "    \n",
    "    # average sales on day of the week for the last 4 or 20 weeks\n",
    "    for i in range(7):\n",
    "        X[f'mean_4_dow{i}_2017'] = get_timespan(df, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X[f'mean_20_dow{i}_2017'] = get_timespan(df, t2017, 140-i, 20, freq='7D').mean(axis=1).values        \n",
    "    \n",
    "    # promotion status of each day 14 days before and 14 days after the reference date\n",
    "    \n",
    "    for i in range(-14, 15):\n",
    "        X[f'promo_{i}'] = promo_df[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    if name_prefix is not None:\n",
    "        X.columns = [f'{name_prefix}_{c}' for c in X.columns]\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(t2017, periods=15)].values\n",
    "        return X, y\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create training data\n",
    "\n",
    "t2017 = date(2017, 5, 30)\n",
    "num_days = 4\n",
    "X_l, y_l = [], []\n",
    "for i in tqdm(range(num_days)):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(df_2017, promo_2017, t2017 + delta)\n",
    "\n",
    "    X_tmp = pd.concat([X_tmp, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "\n",
    "del X_l, y_l\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data\n",
    "\n",
    "X_val, y_val = prepare_dataset(df_2017, promo_2017, date(2017, 7, 11))\n",
    "\n",
    "X_val = pd.concat([X_val, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_test, y_test = prepare_dataset(df_2017, promo_2017, date(2017, 8, 1))\n",
    "\n",
    "X_test = pd.concat([X_test, items.reset_index(drop=True), stores.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670060, 538) (670060, 15)\n",
      "(167515, 538) (167515, 15)\n",
      "(167515, 538) (167515, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_2017, promo_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_test[:] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "X_val = X_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512 x 512 x 256 x 128 x 64 x 32 x 16 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5027 - mse: 0.4735 - val_loss: 0.3926 - val_mse: 0.3926\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3498 - mse: 0.3303 - val_loss: 0.3400 - val_mse: 0.3400\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3345 - mse: 0.3162 - val_loss: 0.3278 - val_mse: 0.3278\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3255 - mse: 0.3079 - val_loss: 0.3245 - val_mse: 0.3245\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3199 - mse: 0.3027 - val_loss: 0.3197 - val_mse: 0.3197\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3151 - mse: 0.2983 - val_loss: 0.3203 - val_mse: 0.3203\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3114 - mse: 0.2948 - val_loss: 0.3214 - val_mse: 0.3214\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3077 - mse: 0.2914 - val_loss: 0.3209 - val_mse: 0.3209\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3045 - mse: 0.2884 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3018 - mse: 0.2859 - val_loss: 0.3247 - val_mse: 0.3247\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.2988 - mse: 0.2831 - val_loss: 0.3239 - val_mse: 0.3239\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2960 - mse: 0.2805 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2810 - mse: 0.2667 - val_loss: 0.3226 - val_mse: 0.3226\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2743 - mse: 0.2606 - val_loss: 0.3239 - val_mse: 0.3239\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2713 - mse: 0.2577 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.4825 - mse: 0.4557 - val_loss: 0.4079 - val_mse: 0.4079\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3458 - mse: 0.3283 - val_loss: 0.3363 - val_mse: 0.3363\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3312 - mse: 0.3149 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3228 - mse: 0.3072 - val_loss: 0.3229 - val_mse: 0.3229\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3173 - mse: 0.3021 - val_loss: 0.3222 - val_mse: 0.3222\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3124 - mse: 0.2975 - val_loss: 0.3206 - val_mse: 0.3206\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3089 - mse: 0.2942 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3052 - mse: 0.2908 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3019 - mse: 0.2876 - val_loss: 0.3237 - val_mse: 0.3237\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.2990 - mse: 0.2849 - val_loss: 0.3243 - val_mse: 0.3243\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.2959 - mse: 0.2820 - val_loss: 0.3266 - val_mse: 0.3266\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2934 - mse: 0.2797 - val_loss: 0.3272 - val_mse: 0.3272\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2907 - mse: 0.2771 - val_loss: 0.3290 - val_mse: 0.3290\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2733 - mse: 0.2609 - val_loss: 0.3251 - val_mse: 0.3251\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2660 - mse: 0.2541 - val_loss: 0.3285 - val_mse: 0.3285\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.2626 - mse: 0.2510 - val_loss: 0.3300 - val_mse: 0.3300\n",
      "Epoch 17/2000\n",
      " - 9s - loss: 0.2601 - mse: 0.2486 - val_loss: 0.3317 - val_mse: 0.3317\n",
      "Epoch 18/2000\n",
      " - 9s - loss: 0.2580 - mse: 0.2466 - val_loss: 0.3338 - val_mse: 0.3338\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5265 - mse: 0.4962 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3724 - mse: 0.3519 - val_loss: 0.3852 - val_mse: 0.3852\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3587 - mse: 0.3390 - val_loss: 0.3618 - val_mse: 0.3618\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3502 - mse: 0.3312 - val_loss: 0.3506 - val_mse: 0.3506\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3438 - mse: 0.3252 - val_loss: 0.3498 - val_mse: 0.3498\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3395 - mse: 0.3212 - val_loss: 0.3471 - val_mse: 0.3471\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3351 - mse: 0.3171 - val_loss: 0.3479 - val_mse: 0.3479\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3310 - mse: 0.3133 - val_loss: 0.3550 - val_mse: 0.3550\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3275 - mse: 0.3100 - val_loss: 0.3541 - val_mse: 0.3541\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3246 - mse: 0.3073 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3211 - mse: 0.3040 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3182 - mse: 0.3014 - val_loss: 0.3549 - val_mse: 0.3549\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3156 - mse: 0.2989 - val_loss: 0.3604 - val_mse: 0.3604\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2974 - mse: 0.2820 - val_loss: 0.3540 - val_mse: 0.3540\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.2891 - mse: 0.2743 - val_loss: 0.3562 - val_mse: 0.3562\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.2851 - mse: 0.2706 - val_loss: 0.3590 - val_mse: 0.3590\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.4766 - mse: 0.4500 - val_loss: 0.3988 - val_mse: 0.3988\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3677 - mse: 0.3487 - val_loss: 0.3531 - val_mse: 0.3531\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3537 - mse: 0.3358 - val_loss: 0.3458 - val_mse: 0.3458\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3449 - mse: 0.3277 - val_loss: 0.3458 - val_mse: 0.3458\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3390 - mse: 0.3221 - val_loss: 0.3462 - val_mse: 0.3462\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3341 - mse: 0.3176 - val_loss: 0.3557 - val_mse: 0.3557\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3295 - mse: 0.3133 - val_loss: 0.3457 - val_mse: 0.3457\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3263 - mse: 0.3103 - val_loss: 0.3464 - val_mse: 0.3464\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3225 - mse: 0.3068 - val_loss: 0.3504 - val_mse: 0.3504\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3192 - mse: 0.3036 - val_loss: 0.3498 - val_mse: 0.3498\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3029 - mse: 0.2886 - val_loss: 0.3463 - val_mse: 0.3463\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2960 - mse: 0.2821 - val_loss: 0.3476 - val_mse: 0.3476\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.2927 - mse: 0.2790 - val_loss: 0.3496 - val_mse: 0.3496\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2901 - mse: 0.2766 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 15/2000\n",
      " - 10s - loss: 0.2877 - mse: 0.2744 - val_loss: 0.3536 - val_mse: 0.3536\n",
      "Epoch 16/2000\n",
      " - 10s - loss: 0.2856 - mse: 0.2724 - val_loss: 0.3557 - val_mse: 0.3557\n",
      "Epoch 17/2000\n",
      " - 10s - loss: 0.2835 - mse: 0.2705 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5195 - mse: 0.4911 - val_loss: 0.4472 - val_mse: 0.4472\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3914 - mse: 0.3703 - val_loss: 0.3781 - val_mse: 0.3781\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3768 - mse: 0.3567 - val_loss: 0.3687 - val_mse: 0.3687\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3676 - mse: 0.3481 - val_loss: 0.3679 - val_mse: 0.3679\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3610 - mse: 0.3420 - val_loss: 0.3673 - val_mse: 0.3673\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3555 - mse: 0.3369 - val_loss: 0.3699 - val_mse: 0.3699\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3504 - mse: 0.3321 - val_loss: 0.3689 - val_mse: 0.3689\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3460 - mse: 0.3280 - val_loss: 0.3715 - val_mse: 0.3715\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3424 - mse: 0.3247 - val_loss: 0.3700 - val_mse: 0.3700\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3381 - mse: 0.3207 - val_loss: 0.3752 - val_mse: 0.3752\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3348 - mse: 0.3176 - val_loss: 0.3766 - val_mse: 0.3766\n",
      "Epoch 12/2000\n",
      " - 10s - loss: 0.3307 - mse: 0.3138 - val_loss: 0.3864 - val_mse: 0.3864\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3110 - mse: 0.2955 - val_loss: 0.3742 - val_mse: 0.3742\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3020 - mse: 0.2872 - val_loss: 0.3776 - val_mse: 0.3776\n",
      "Epoch 15/2000\n",
      " - 10s - loss: 0.2977 - mse: 0.2832 - val_loss: 0.3794 - val_mse: 0.3794\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5552 - mse: 0.5235 - val_loss: 0.4373 - val_mse: 0.4373\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.4041 - mse: 0.3823 - val_loss: 0.3930 - val_mse: 0.3930\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3864 - mse: 0.3659 - val_loss: 0.3840 - val_mse: 0.3840\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3754 - mse: 0.3556 - val_loss: 0.3779 - val_mse: 0.3779\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3683 - mse: 0.3490 - val_loss: 0.3814 - val_mse: 0.3814\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3616 - mse: 0.3428 - val_loss: 0.3788 - val_mse: 0.3788\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3566 - mse: 0.3382 - val_loss: 0.3816 - val_mse: 0.3816\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3523 - mse: 0.3341 - val_loss: 0.3837 - val_mse: 0.3837\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3475 - mse: 0.3297 - val_loss: 0.3842 - val_mse: 0.3842\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3432 - mse: 0.3257 - val_loss: 0.3852 - val_mse: 0.3852\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3395 - mse: 0.3223 - val_loss: 0.3896 - val_mse: 0.3896\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3199 - mse: 0.3040 - val_loss: 0.3863 - val_mse: 0.3863\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3114 - mse: 0.2962 - val_loss: 0.3888 - val_mse: 0.3888\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3073 - mse: 0.2924 - val_loss: 0.3914 - val_mse: 0.3914\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.4836 - mse: 0.4550 - val_loss: 0.4731 - val_mse: 0.4731\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.3858 - mse: 0.3637 - val_loss: 0.3814 - val_mse: 0.3814\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3737 - mse: 0.3525 - val_loss: 0.3671 - val_mse: 0.3671\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3663 - mse: 0.3457 - val_loss: 0.3663 - val_mse: 0.3663\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3601 - mse: 0.3400 - val_loss: 0.3692 - val_mse: 0.3692\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3553 - mse: 0.3356 - val_loss: 0.3762 - val_mse: 0.3762\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3510 - mse: 0.3316 - val_loss: 0.3704 - val_mse: 0.3704\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3470 - mse: 0.3279 - val_loss: 0.3733 - val_mse: 0.3733\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3424 - mse: 0.3236 - val_loss: 0.3752 - val_mse: 0.3752\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3392 - mse: 0.3206 - val_loss: 0.3746 - val_mse: 0.3746\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3355 - mse: 0.3172 - val_loss: 0.3790 - val_mse: 0.3790\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3170 - mse: 0.3001 - val_loss: 0.3818 - val_mse: 0.3818\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.3085 - mse: 0.2922 - val_loss: 0.3831 - val_mse: 0.3831\n",
      "Epoch 14/2000\n",
      " - 10s - loss: 0.3044 - mse: 0.2884 - val_loss: 0.3868 - val_mse: 0.3868\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5455 - mse: 0.5143 - val_loss: 0.4090 - val_mse: 0.4090\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.3817 - mse: 0.3609 - val_loss: 0.3687 - val_mse: 0.3687\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3649 - mse: 0.3454 - val_loss: 0.3626 - val_mse: 0.3626\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3558 - mse: 0.3370 - val_loss: 0.3653 - val_mse: 0.3653\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3486 - mse: 0.3304 - val_loss: 0.3608 - val_mse: 0.3608\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3433 - mse: 0.3254 - val_loss: 0.3631 - val_mse: 0.3631\n",
      "Epoch 7/2000\n",
      " - 10s - loss: 0.3390 - mse: 0.3214 - val_loss: 0.3639 - val_mse: 0.3639\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3351 - mse: 0.3178 - val_loss: 0.3646 - val_mse: 0.3646\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3319 - mse: 0.3148 - val_loss: 0.3666 - val_mse: 0.3666\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3282 - mse: 0.3113 - val_loss: 0.3698 - val_mse: 0.3698\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3253 - mse: 0.3086 - val_loss: 0.3711 - val_mse: 0.3711\n",
      "Epoch 12/2000\n",
      " - 10s - loss: 0.3217 - mse: 0.3052 - val_loss: 0.3733 - val_mse: 0.3733\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.3036 - mse: 0.2884 - val_loss: 0.3695 - val_mse: 0.3695\n",
      "Epoch 14/2000\n",
      " - 10s - loss: 0.2961 - mse: 0.2814 - val_loss: 0.3715 - val_mse: 0.3715\n",
      "Epoch 15/2000\n",
      " - 10s - loss: 0.2925 - mse: 0.2781 - val_loss: 0.3741 - val_mse: 0.3741\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 11s - loss: 0.5088 - mse: 0.4797 - val_loss: 0.4044 - val_mse: 0.4044\n",
      "Epoch 2/2000\n",
      " - 9s - loss: 0.3678 - mse: 0.3492 - val_loss: 0.3627 - val_mse: 0.3627\n",
      "Epoch 3/2000\n",
      " - 9s - loss: 0.3492 - mse: 0.3325 - val_loss: 0.3535 - val_mse: 0.3535\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3401 - mse: 0.3241 - val_loss: 0.3514 - val_mse: 0.3514\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3338 - mse: 0.3182 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3287 - mse: 0.3135 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3251 - mse: 0.3100 - val_loss: 0.3516 - val_mse: 0.3516\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3210 - mse: 0.3062 - val_loss: 0.3546 - val_mse: 0.3546\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3180 - mse: 0.3034 - val_loss: 0.3556 - val_mse: 0.3556\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3145 - mse: 0.3001 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3116 - mse: 0.2973 - val_loss: 0.3599 - val_mse: 0.3599\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.2951 - mse: 0.2820 - val_loss: 0.3558 - val_mse: 0.3558\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2881 - mse: 0.2755 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.2847 - mse: 0.2723 - val_loss: 0.3597 - val_mse: 0.3597\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.4729 - mse: 0.4461 - val_loss: 0.4193 - val_mse: 0.4193\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.3707 - mse: 0.3502 - val_loss: 0.3741 - val_mse: 0.3741\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3579 - mse: 0.3384 - val_loss: 0.3673 - val_mse: 0.3673\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3506 - mse: 0.3316 - val_loss: 0.3680 - val_mse: 0.3680\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3450 - mse: 0.3264 - val_loss: 0.3698 - val_mse: 0.3698\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3404 - mse: 0.3222 - val_loss: 0.3714 - val_mse: 0.3714\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3362 - mse: 0.3183 - val_loss: 0.3704 - val_mse: 0.3704\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3326 - mse: 0.3149 - val_loss: 0.3706 - val_mse: 0.3706\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3289 - mse: 0.3114 - val_loss: 0.3720 - val_mse: 0.3720\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3252 - mse: 0.3080 - val_loss: 0.3780 - val_mse: 0.3780\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3087 - mse: 0.2928 - val_loss: 0.3754 - val_mse: 0.3754\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3011 - mse: 0.2857 - val_loss: 0.3770 - val_mse: 0.3770\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.2975 - mse: 0.2823 - val_loss: 0.3760 - val_mse: 0.3760\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5177 - mse: 0.4890 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.3780 - mse: 0.3586 - val_loss: 0.3884 - val_mse: 0.3884\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3609 - mse: 0.3429 - val_loss: 0.3723 - val_mse: 0.3723\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3514 - mse: 0.3341 - val_loss: 0.3690 - val_mse: 0.3690\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3442 - mse: 0.3274 - val_loss: 0.3703 - val_mse: 0.3703\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3392 - mse: 0.3227 - val_loss: 0.3710 - val_mse: 0.3710\n",
      "Epoch 7/2000\n",
      " - 10s - loss: 0.3344 - mse: 0.3182 - val_loss: 0.3720 - val_mse: 0.3720\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3292 - mse: 0.3134 - val_loss: 0.3730 - val_mse: 0.3730\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3263 - mse: 0.3107 - val_loss: 0.3756 - val_mse: 0.3756\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3225 - mse: 0.3071 - val_loss: 0.3773 - val_mse: 0.3773\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3194 - mse: 0.3042 - val_loss: 0.3763 - val_mse: 0.3763\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 10s - loss: 0.3008 - mse: 0.2869 - val_loss: 0.3731 - val_mse: 0.3731\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.2927 - mse: 0.2793 - val_loss: 0.3757 - val_mse: 0.3757\n",
      "Epoch 14/2000\n",
      " - 10s - loss: 0.2891 - mse: 0.2760 - val_loss: 0.3773 - val_mse: 0.3773\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5626 - mse: 0.5310 - val_loss: 0.4611 - val_mse: 0.4611\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.4123 - mse: 0.3904 - val_loss: 0.4113 - val_mse: 0.4113\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3955 - mse: 0.3748 - val_loss: 0.4013 - val_mse: 0.4013\n",
      "Epoch 4/2000\n",
      " - 9s - loss: 0.3851 - mse: 0.3650 - val_loss: 0.4037 - val_mse: 0.4037\n",
      "Epoch 5/2000\n",
      " - 9s - loss: 0.3775 - mse: 0.3580 - val_loss: 0.4028 - val_mse: 0.4028\n",
      "Epoch 6/2000\n",
      " - 9s - loss: 0.3709 - mse: 0.3518 - val_loss: 0.3999 - val_mse: 0.3999\n",
      "Epoch 7/2000\n",
      " - 9s - loss: 0.3660 - mse: 0.3473 - val_loss: 0.4063 - val_mse: 0.4063\n",
      "Epoch 8/2000\n",
      " - 9s - loss: 0.3614 - mse: 0.3430 - val_loss: 0.4086 - val_mse: 0.4086\n",
      "Epoch 9/2000\n",
      " - 9s - loss: 0.3569 - mse: 0.3387 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 10/2000\n",
      " - 9s - loss: 0.3533 - mse: 0.3354 - val_loss: 0.4125 - val_mse: 0.4125\n",
      "Epoch 11/2000\n",
      " - 9s - loss: 0.3495 - mse: 0.3319 - val_loss: 0.4175 - val_mse: 0.4175\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3450 - mse: 0.3277 - val_loss: 0.4221 - val_mse: 0.4221\n",
      "Epoch 13/2000\n",
      " - 9s - loss: 0.3420 - mse: 0.3249 - val_loss: 0.4221 - val_mse: 0.4221\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/2000\n",
      " - 9s - loss: 0.3195 - mse: 0.3040 - val_loss: 0.4132 - val_mse: 0.4132\n",
      "Epoch 15/2000\n",
      " - 9s - loss: 0.3103 - mse: 0.2955 - val_loss: 0.4161 - val_mse: 0.4161\n",
      "Epoch 16/2000\n",
      " - 9s - loss: 0.3059 - mse: 0.2914 - val_loss: 0.4194 - val_mse: 0.4194\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5268 - mse: 0.4976 - val_loss: 0.4846 - val_mse: 0.4846\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.4185 - mse: 0.3962 - val_loss: 0.4212 - val_mse: 0.4212\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.4014 - mse: 0.3804 - val_loss: 0.4186 - val_mse: 0.4186\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3898 - mse: 0.3696 - val_loss: 0.4100 - val_mse: 0.4100\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3820 - mse: 0.3623 - val_loss: 0.4093 - val_mse: 0.4093\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3753 - mse: 0.3561 - val_loss: 0.4097 - val_mse: 0.4097\n",
      "Epoch 7/2000\n",
      " - 10s - loss: 0.3693 - mse: 0.3506 - val_loss: 0.4101 - val_mse: 0.4101\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3644 - mse: 0.3460 - val_loss: 0.4098 - val_mse: 0.4098\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3596 - mse: 0.3415 - val_loss: 0.4148 - val_mse: 0.4148\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3546 - mse: 0.3368 - val_loss: 0.4172 - val_mse: 0.4172\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3505 - mse: 0.3330 - val_loss: 0.4249 - val_mse: 0.4249\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3468 - mse: 0.3295 - val_loss: 0.4272 - val_mse: 0.4272\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.3241 - mse: 0.3084 - val_loss: 0.4190 - val_mse: 0.4190\n",
      "Epoch 14/2000\n",
      " - 10s - loss: 0.3142 - mse: 0.2993 - val_loss: 0.4221 - val_mse: 0.4221\n",
      "Epoch 15/2000\n",
      " - 10s - loss: 0.3096 - mse: 0.2950 - val_loss: 0.4262 - val_mse: 0.4262\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5162 - mse: 0.4857 - val_loss: 0.4518 - val_mse: 0.4518\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.4029 - mse: 0.3802 - val_loss: 0.4115 - val_mse: 0.4115\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3885 - mse: 0.3668 - val_loss: 0.4041 - val_mse: 0.4041\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3798 - mse: 0.3588 - val_loss: 0.4048 - val_mse: 0.4048\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3735 - mse: 0.3530 - val_loss: 0.4073 - val_mse: 0.4073\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3679 - mse: 0.3478 - val_loss: 0.4099 - val_mse: 0.4099\n",
      "Epoch 7/2000\n",
      " - 10s - loss: 0.3634 - mse: 0.3436 - val_loss: 0.4103 - val_mse: 0.4103\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3587 - mse: 0.3392 - val_loss: 0.4121 - val_mse: 0.4121\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3546 - mse: 0.3354 - val_loss: 0.4188 - val_mse: 0.4188\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3502 - mse: 0.3313 - val_loss: 0.4173 - val_mse: 0.4173\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3317 - mse: 0.3142 - val_loss: 0.4138 - val_mse: 0.4138\n",
      "Epoch 12/2000\n",
      " - 10s - loss: 0.3235 - mse: 0.3067 - val_loss: 0.4150 - val_mse: 0.4150\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.3193 - mse: 0.3028 - val_loss: 0.4166 - val_mse: 0.4166\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Train on 670060 samples, validate on 167515 samples\n",
      "Epoch 1/2000\n",
      " - 12s - loss: 0.5550 - mse: 0.5228 - val_loss: 0.4628 - val_mse: 0.4628\n",
      "Epoch 2/2000\n",
      " - 10s - loss: 0.3993 - mse: 0.3773 - val_loss: 0.4059 - val_mse: 0.4059\n",
      "Epoch 3/2000\n",
      " - 10s - loss: 0.3819 - mse: 0.3612 - val_loss: 0.3934 - val_mse: 0.3934\n",
      "Epoch 4/2000\n",
      " - 10s - loss: 0.3719 - mse: 0.3520 - val_loss: 0.3917 - val_mse: 0.3917\n",
      "Epoch 5/2000\n",
      " - 10s - loss: 0.3648 - mse: 0.3454 - val_loss: 0.3946 - val_mse: 0.3946\n",
      "Epoch 6/2000\n",
      " - 10s - loss: 0.3598 - mse: 0.3407 - val_loss: 0.3938 - val_mse: 0.3938\n",
      "Epoch 7/2000\n",
      " - 10s - loss: 0.3544 - mse: 0.3357 - val_loss: 0.3966 - val_mse: 0.3966\n",
      "Epoch 8/2000\n",
      " - 10s - loss: 0.3509 - mse: 0.3325 - val_loss: 0.3963 - val_mse: 0.3963\n",
      "Epoch 9/2000\n",
      " - 10s - loss: 0.3468 - mse: 0.3286 - val_loss: 0.3992 - val_mse: 0.3992\n",
      "Epoch 10/2000\n",
      " - 10s - loss: 0.3428 - mse: 0.3249 - val_loss: 0.4005 - val_mse: 0.4005\n",
      "Epoch 11/2000\n",
      " - 10s - loss: 0.3396 - mse: 0.3219 - val_loss: 0.4027 - val_mse: 0.4027\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/2000\n",
      " - 9s - loss: 0.3222 - mse: 0.3058 - val_loss: 0.3977 - val_mse: 0.3977\n",
      "Epoch 13/2000\n",
      " - 10s - loss: 0.3141 - mse: 0.2984 - val_loss: 0.3986 - val_mse: 0.3986\n",
      "Epoch 14/2000\n",
      " - 10s - loss: 0.3106 - mse: 0.2951 - val_loss: 0.4006 - val_mse: 0.4006\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2000\n",
    "\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * num_days) * 0.25 + 1 )\n",
    "for i in range(15):\n",
    "    print(\"=\" * 50)\n",
    "    print(f'Step {i+1}')\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    y_mean = y.mean()\n",
    "    xv = X_val\n",
    "    yv = y_val[:, i]\n",
    "    model = build_model()\n",
    "\n",
    "    opt = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "        ]\n",
    "    \n",
    "    #smaller batch size runs faster\n",
    "    #batch_size = 65536\n",
    "    batch_size = 8192\n",
    "\n",
    "    model.fit(X_train, y - y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv-y_mean), callbacks=callbacks )\n",
    "    val_pred.append(model.predict(X_val) + y_mean)\n",
    "    test_pred.append(model.predict(X_test) + y_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation nwrmsle = 0.6158113158278086\n"
     ]
    }
   ],
   "source": [
    "weight = items[\"perishable\"] * 0.25 + 1\n",
    "val_err = (y_val - np.array(val_pred).squeeze(axis=2).transpose())**2\n",
    "val_err = val_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "val_err = np.sqrt(val_err.sum() / weight.sum() / 15)\n",
    "print(f'validation nwrmsle = {val_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nwrmsle = 0.6284861251709798\n"
     ]
    }
   ],
   "source": [
    "test_err = (y_test - np.array(test_pred).squeeze(axis=2).transpose())**2\n",
    "test_err = test_err.sum(axis=1) * weight\n",
    "#change to 15 days\n",
    "test_err = np.sqrt(test_err.sum() / weight.sum() / 15)\n",
    "print(f'test nwrmsle = {test_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test = pd.DataFrame(np.array(test_pred).squeeze(axis=2).transpose(), \n",
    "                            index = df_2017_index, columns = pd.date_range('2017-08-01',periods=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../model_results/2020-01-08/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.DataFrame({'x':[1,2,3],'y':[1,2,3]})\n",
    "df_check.to_csv(out_path + 'check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test.to_csv(out_path + 'nn_test_pred_model_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
